{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CarND-Advanced-Lane-Lines\n",
    "## Advanced Lane Finding Project\n",
    "#### The goals / steps of this project are the following:\n",
    "\n",
    "- Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "- Apply a distortion correction to raw images.\n",
    "- Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "- Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "- Detect lane pixels and fit to find the lane boundary.\n",
    "- Determine the curvature of the lane and vehicle position with respect to center.\n",
    "- Warp the detected lane boundaries back onto the original image.\n",
    "- Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "## Writeup / README\n",
    "1. Provide a Writeup / README that includes all the rubric points and how you addressed each one. You can submit your writeup as markdown or pdf. Here is a template writeup for this project you can use as a guide and a starting point.\n",
    "\n",
    "You're reading it!\n",
    "## Camera Calibration\n",
    "\n",
    "To calibrate camera I used 20 images of chessboard taken from different angels and distance. Images where provided in project repository.\n",
    "\n",
    "For every image:\n",
    "\n",
    "- I change it from color to grayscale,\n",
    "- with counted number of inner corners of chessboard in any given row (9) and column (6) I use OpenCV functions findChessboardCorners() to find object points (3d points in real world space) and image points (2d points in image plane),\n",
    "- to test results I draw points on images with drawChessboardCorners() function.\n",
    "\n",
    "After I loop all images, I can get camera calibration and distortion correction with OpenCV function cv2.calibrateCamera() and previously counted image points and object points. Results - camera matrix and and vector of distortion coefficients are saved to file.\n",
    "\n",
    "To check if everything goes correctly it can be used funtion cv2.undistort(). Example image before calibration: \n",
    "![title](./camera_cal/calibration2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calibration and distortion correction: \n",
    "![title](./output_images/cam_calibrated_img.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Pipeline (single images)\n",
    "\n",
    "### 1. Provide an example of a distortion-corrected image.\n",
    "Distortion-corrected image is a result of OpenCV function undistort(). \n",
    "![title](./output_images/bothImg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe how (and identify where in your code) you used color transforms, gradients or other methods to create a thresholded binary image. Provide an example of a binary image result.\n",
    "\n",
    "In my code I defined functions to get different variants of gradient and color thresholding. In gradient threshold I used cv2.Sobel() function, with different directions and magniute of gradient, like it was showed during the course. In color threshold first I created functions with color spaces showed at course - RGB and HLS. But I couldn't get satisfying results so after reading about color thresholding in the Internet I added functions, which use also Lab and LUV color spaces.\n",
    "\n",
    "After a lot of test with different functions and thresh values I used only three of defined funcions - color_s_HLS_threshold(image, thresh=(180,240)), color_b_Lab_threshold(image, thresh=(200,255)) and color_l_LUV_threshold(image, thresh=(200,255)). Final combined thresholding, which is used in pipeline, is in funcion threshold(image).\n",
    "![title](./output_images/thresh_img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Describe how (and identify where in your code) you performed a perspective transform and provide an example of a transformed image.\n",
    "\n",
    "The code for my perspective transform includes a function called warp_image(img). The function takes as inputs an image (img), as well as source (src) and destination (dst) points. I chose the hardcode the source and destination points in the following manner:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    src = np.float32([(560,453), (720,453), \n",
    "                  (170,720), (1150,720)])\n",
    "    dst = np.float32([(80,0), (1200,0),\n",
    "                  (80,720), (1200,720)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the perspective transform M and inverse perspective transform Minv I used OpenCV function cv2.getPerspectiveTransform(). \n",
    "![title](./output_images/warp_img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Describe how (and identify where in your code) you identified lane-line pixels and fit their positions with a polynomial?\n",
    "#### 4.1 Sliding window lines finding\n",
    "\n",
    "First, I take a histogram along all the columns in the lower half of image. Because it's binary image and all pixels are either 0 or 1 historgram shows in which columns there are most values equal 1. Two most prominent peaks are be good indicators of the x-position of the base of the lane lines. From that point I use a sliding window, placed around the line centers, to find and follow the lines up to the top of the frame. I divide height on 9 slides and starting from the bottom one I identify the nonzero pixels in x and y within the window. If I find more white pixels than fixed minimum value (50 in my case) I recenter sliding window. When I go throught all sliedes I calculate polynomial of degree 2, which determines found line. In my code funcion sliding_window_lines_finding() is performing this operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./output_images/histogram.png)\n",
    "![title](./output_images/sliding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Highly targeted search\n",
    "\n",
    "During video pipeline I calculate lane line by getting average from 5 last frames. If I don't have lane lines from any of last 5 frames I'm using sliding_window_lines_finding() function, but if I have determined lines I change method of findig line on current frame. This is done by funcdion highly_targeted_lines_finding() in code. Now I'm looking for new lines in a margin (100 pixels, hardcoded) around the previous line position. \n",
    "![title](./output_images/highsliding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Describe how (and identify where in your code) you calculated the radius of curvature of the lane and the position of the vehicle with respect to center.\n",
    "\n",
    "Funcion calculate_radius_and_meters_from_center() is calculating both radius of curvature and vehicles position to center.\n",
    "\n",
    "#### 5.1 Radius of curvature\n",
    "\n",
    "Lane lines are calculated as second order polynomial curve, given by equation:\n",
    "f(y) = A*y^2 + B*y + C.\n",
    "And the equation of it radius is: \n",
    "r = (1 + (2*A*y + B)^2)^(3/2)/abs(2*A).\n",
    "\n",
    "Image size is given in pixel values so it is needed to change pixels to meters by multiply it by hardcoded factors - meters per pixel in y and x dimension.\n",
    "Because I wanted to measure the radius of curvature closest to vehicle, I evaluate the formula  at the y value corresponding to the bottom of your image. \n",
    "I calculate left and right line radius and then get average of them as final result.\n",
    "\n",
    "#### 5.2 Position of the vehicle with respect to center\n",
    "\n",
    "I'm taking y value at the bottom of image of left and right line. Then I calculate car middle position and it distance from  center of image. This gives vehicles position in pixels so I multiply it by hardcoded factor - meters per pixel in y dimension.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Provide an example image of your result plotted back down onto the road such that the lane area is identified clearly.\n",
    "![title](./output_images/radius_and_lines.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline (video)\n",
    "### 1. Provide a link to your final video output. Your pipeline should perform reasonably well on the entire project video (wobbly lines are ok but no catastrophic failures that would cause the car to drive off the road!).\n",
    "\n",
    "[Link to video](project_output_video.mp4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "### 1. Briefly discuss any problems / issues you faced in your implementation of this project. Where will your pipeline likely fail? What could you do to make it more robust?\n",
    "\n",
    "My main problem was to find good thresholding. It's quite easy to get lane lines from one image. But in video, when the  lighting, shadows are changing it's much harder. Also two colors of lines, yellow and white, makes it a bit harder.\n",
    "\n",
    "Finally I've got good results at project video, but it was recorded in normal, sunny day with good weather conditions. I think that any weather change (rain, snow, fog) or time change (to evening or night) will be a challenge to my pipeline, on which it can fail.\n",
    "\n",
    "I believe that therholding fucntion still can be upgraded and gives better results. Also I think that lane lines curviture can be calculated with better accuracy, if I'll use more advanced method than quadratic function."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
